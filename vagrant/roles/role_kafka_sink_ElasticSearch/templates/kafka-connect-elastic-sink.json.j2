{
  "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
  "value.converter": "io.confluent.connect.avro.AvroConverter",
  "key.converter": "io.confluent.connect.avro.AvroConverter",
  "key.converter.schema.registry.url": "http://{{ kafka_service_ip_address }}:{{ port_kafka_schema_registry }}",
  "value.converter.schema.registry.url": "http://{{ kafka_service_ip_address }}:{{ port_kafka_schema_registry }}",

  "_comment": "Which topic to stream data from into Elasticsearch",
  "topics": "bulk-logs",

  "transforms": "dropPrefix,routeTS",
  "transforms.dropPrefix.type": "org.apache.kafka.connect.transforms.RegexRouter",
  "transforms.dropPrefix.regex": "bulk-(.*)",
  "transforms.dropPrefix.replacement": "$1",
  "transforms.routeTS.type": "org.apache.kafka.connect.transforms.TimestampRouter",
  "transforms.routeTS.topic.format": "kafka-${topic}-${timestamp}",
  "transforms.routeTS.timestamp.format": "YYYYMMdd",
  "log.message.timestamp.type": "LogAppendTime",
  "replication.factor": 3,

  "_comment": "--- Elasticsearch-specific config ---",
  "_comment": "Elasticsearch server address",
  "connection.url": "{{ elastic_server_url }}",

  "_comment": "Elasticsearch mapping name. Gets created automatically if doesn't exist  ",
  "type.name": "type.name=kafka-connect",

  "_comment": "If the Kafka message doesn't have a key (as is the case with JDBC source)  you need to specify key.ignore=true. If you don't, you'll get an error from the Connect task: 'ConnectException: Key is used as document id and can not be null.",
  "key.ignore": "true"
}
